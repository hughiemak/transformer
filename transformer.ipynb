{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "09d13e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pdb import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7d670f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, temperature, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, q, k, v):\n",
    "        attn = torch.matmul(q / self.temperature, k.transpose(-2,-1))\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        output = self.dropout(torch.matmul(attn, v))\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b29c0669",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        assert self.d_k==self.d_v and self.d_k*self.n_head == self.d_model, \"d_model must equal d_k * n_head and d_v * n_head\"\n",
    "        \n",
    "        self.w_qs = nn.Linear(self.d_model, n_head*d_k, bias=False)\n",
    "        self.w_ks = nn.Linear(self.d_model, n_head*d_k, bias=False)\n",
    "        self.w_vs = nn.Linear(self.d_model, n_head*d_v, bias=False)\n",
    "        self.fc = nn.Linear(n_head * self.d_v, self.d_model, bias=False)\n",
    "        \n",
    "        self.attention = ScaledDotProductAttention(temperature=self.d_k ** 0.5)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(self.d_model, eps=1e-6)\n",
    "    \n",
    "    def forward(self, q, k, v):\n",
    "        batch_size = q.shape[0]\n",
    "        len_q, len_k, len_v = q.shape[1], k.shape[1], v.shape[1]\n",
    "        \n",
    "        residual = q\n",
    "        \n",
    "        q = self.w_qs(q).view(batch_size, len_q, self.n_head, self.d_k)\n",
    "        k = self.w_ks(k).view(batch_size, len_k, self.n_head, self.d_k)\n",
    "        v = self.w_vs(v).view(batch_size, len_v, self.n_head, self.d_v)\n",
    "        \n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2) # Change shape from (b, n, h, d_k/v) to (b, h, n, d_k/v)\n",
    "        \n",
    "        attn_out, attn = self.attention(q, k, v) # attn_out: (b, h, n, d_k), attn: (b, h, n, n)\n",
    "        attn_out = attn_out.transpose(1, 2).contiguous().view(batch_size, len_q, -1) # Change shape from (b, h, n, d_k) to (b, n, h*d_k)\n",
    "        \n",
    "        mh_attn_out = self.fc(attn_out) # (b, n, d_model)\n",
    "        \n",
    "        return self.layer_norm(residual + self.dropout(mh_attn_out)), attn # (b, n, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "debaa9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_in, d_hid, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_in, d_hid)\n",
    "        self.w_2 = nn.Linear(d_hid, d_in)\n",
    "        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.w_2(F.relu(self.w_1(x)))\n",
    "        x = self.dropout(x)\n",
    "        return self.layer_norm(residual + x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1f1c7166",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_inner, n_head, d_k, d_v, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mh_attn_out, mh_att = self.slf_attn(x, x, x)\n",
    "        return self.pos_ffn(mh_attn_out), mh_att\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4d883679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of q, k, v: torch.Size([2, 3, 6])\n",
      "\n",
      "Scaled Dot Product Attention\n",
      "Attention output: \n",
      "torch.Size([2, 3, 6])\n",
      "Attention: \n",
      "torch.Size([2, 3, 3])\n",
      "\n",
      "Multihead Attention\n",
      "Multihead attention output: \n",
      "torch.Size([2, 3, 6])\n",
      "Multihead attention: \n",
      "torch.Size([2, 3, 3, 3])\n",
      "\n",
      "Encoder Layer\n",
      "Encoder layer output: \n",
      "torch.Size([2, 3, 6])\n",
      "Multihead attention: \n",
      "torch.Size([2, 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "n = 3\n",
    "d_model = 6\n",
    "dropout = 0.5\n",
    "n_head = 3\n",
    "d_k = d_v = d_model // n_head\n",
    "d_inner = d_model * 4\n",
    "\n",
    "q = torch.randn((batch_size, n, d_model))\n",
    "k = torch.randn((batch_size, n, d_model))\n",
    "v = torch.randn((batch_size, n, d_model))\n",
    "print(f\"Shape of q, k, v: {q.shape}\")\n",
    "\n",
    "attention = ScaledDotProductAttention(temperature=1.)\n",
    "attn_output, attn = attention(q, k, v)\n",
    "print(\"\\nScaled Dot Product Attention\")\n",
    "print(f\"Attention output: \\n{attn_output.shape}\")\n",
    "print(f\"Attention: \\n{attn.shape}\")\n",
    "\n",
    "print(\"\\nMultihead Attention\")\n",
    "mh_attention = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "mh_att_out, mh_att = mh_attention(q, k, v)\n",
    "print(f\"Multihead attention output: \\n{mh_att_out.shape}\")\n",
    "print(f\"Multihead attention: \\n{mh_att.shape}\")\n",
    "\n",
    "print(\"\\nEncoder Layer\")\n",
    "encoder = EncoderLayer(d_model, d_inner, n_head, d_k, d_v, dropout=dropout)\n",
    "encoder_layer_out, mh_att = encoder(q)\n",
    "print(f\"Encoder layer output: \\n{encoder_layer_out.shape}\")\n",
    "print(f\"Multihead attention: \\n{mh_att.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d400a20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3980a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
