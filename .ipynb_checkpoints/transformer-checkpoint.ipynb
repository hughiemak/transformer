{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09d13e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pdb import set_trace\n",
    "\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b2950f",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention\n",
    "$$\\text{Attention}(Q,K,V)=\\text{softmax}\\left (\\frac{QK^\\top}{\\sqrt{d_k}}V \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d670f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, temperature, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        attn = torch.matmul(q / self.temperature, k.transpose(-2,-1))\n",
    "        if mask is not None: \n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        output = self.dropout(torch.matmul(attn, v))\n",
    "        return output, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c316c4",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "$$\\text{MultiHead}(Q,K,V)=\\text{Concat}(H_1,\\dots,H_h) W^O$$\n",
    "\n",
    "where $H_i=\\text{Attention}(QW_i^Q,KW_i^K,VW_i^V)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b29c0669",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_head = n_head\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        assert self.d_k==self.d_v and self.d_k*self.n_head == self.d_model, \"d_model must equal d_k * n_head and d_v * n_head\"\n",
    "        \n",
    "        self.w_qs = nn.Linear(self.d_model, n_head*d_k, bias=False)\n",
    "        self.w_ks = nn.Linear(self.d_model, n_head*d_k, bias=False)\n",
    "        self.w_vs = nn.Linear(self.d_model, n_head*d_v, bias=False)\n",
    "        self.fc = nn.Linear(n_head * self.d_v, self.d_model, bias=False)\n",
    "        \n",
    "        self.attention = ScaledDotProductAttention(temperature=self.d_k ** 0.5)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(self.d_model, eps=1e-6)\n",
    "    \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.shape[0]\n",
    "        len_q, len_k, len_v = q.shape[1], k.shape[1], v.shape[1]\n",
    "        \n",
    "        residual = q\n",
    "        \n",
    "        q = self.w_qs(q).view(batch_size, len_q, self.n_head, self.d_k)\n",
    "        k = self.w_ks(k).view(batch_size, len_k, self.n_head, self.d_k)\n",
    "        v = self.w_vs(v).view(batch_size, len_v, self.n_head, self.d_v)\n",
    "        \n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2) # Change shape from (b, n, h, d_k/v) to (b, h, n, d_k/v)\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            \n",
    "        attn_out, attn = self.attention(q, k, v, mask) # attn_out: (b, h, n, d_v), attn: (b, h, n, n)\n",
    "        attn_out = attn_out.transpose(1, 2).contiguous().view(batch_size, len_q, -1) # Change shape from (b, h, n, d_v) to (b, n, h*d_v)\n",
    "        \n",
    "        mh_attn_out = self.fc(attn_out) # (b, n, d_model)\n",
    "        \n",
    "        return self.layer_norm(residual + self.dropout(mh_attn_out)), attn # (b, n, d_model), (b, h, n, n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b061a34",
   "metadata": {},
   "source": [
    "## Position-wise Feed-Forward Networks\n",
    "$$\\text{FFN}(x)=\\max(0,xW_1+b_1)W_2+b_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "debaa9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.w_2(F.relu(self.w_1(x)))\n",
    "        x = self.dropout(x)\n",
    "        return self.layer_norm(residual + x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba753d2",
   "metadata": {},
   "source": [
    "## Encoder Layer\n",
    "$$\\text{EncoderLayer}(x)=\\text{FFN}(\\text{MultiHead}(x,x,x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f1c7166",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, n_head, d_k, d_v, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn = PositionwiseFeedForward(d_model, d_ff, dropout=dropout)\n",
    "    \n",
    "    def forward(self, x, slf_attn_mask=None):\n",
    "        mh_attn_out, mh_att = self.slf_attn(x, x, x, slf_attn_mask)\n",
    "        return self.pos_ffn(mh_attn_out), mh_att\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abd0a80",
   "metadata": {},
   "source": [
    "## Decoder Layer\n",
    "$$\\text{DecoderLayer}(x, e) = \\text{FFN}(\\text{MultiHead}(\\text{MultiHead}(x,x,x),e,e))$$\n",
    "\n",
    "where $e$ is the encoder output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "65c58740",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, n_head, d_k, d_v, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.enc_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n",
    "        self.pos_ffn = PositionwiseFeedForward(d_model, d_ff, dropout=dropout)\n",
    "        \n",
    "    def forward(self, dec_input, enc_output, slf_attn_mask=None, dec_enc_attn_mask=None):\n",
    "        dec_output, dec_slf_attn = self.slf_attn(\n",
    "            dec_input, dec_input, dec_input, mask=slf_attn_mask)\n",
    "        dec_output, dec_enc_attn = self.enc_attn(\n",
    "            dec_output, enc_output, enc_output, mask=dec_enc_attn_mask)\n",
    "        dec_output = self.pos_ffn(dec_output)\n",
    "        return dec_output, dec_slf_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f42396",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "$$\\text{PE}(pos, i)=\\begin{cases}sin(pos/10000^{i/d_{model}}) & \\text{if } i \\text{ is even}\\\\\\cos(pos/10000^{i/d_{model}}) & \\text{otherwise}\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b20fd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, n_position=200):\n",
    "        super().__init__()\n",
    "        self.register_buffer('pos_table', self._get_sinusoid_encoding_table(n_position, d_model)) # not a parameter\n",
    "        \n",
    "    def _get_sinusoid_encoding_table(self, n_position, d_model):\n",
    "        pe = torch.zeros(n_position, d_model)\n",
    "        position = torch.arange(0, n_position).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        return torch.FloatTensor(pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pos_table[:, :x.size(1)].clone().detach()\n",
    "    \n",
    "    def visualize(self):\n",
    "        plt.title(\"Positional Encoding Visualization\")\n",
    "        plt.imshow(self.pos_table.squeeze(), cmap='Blues')\n",
    "        plt.ylabel('Position')\n",
    "        plt.xlabel('Dimension')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6256c8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PositionalEncoding(100, 30).visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8501eda",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "$$\\text{Encoder}(x) = \\text{EncoderLayer}_N(\\dots(\\text{EncoderLayer}_1(x+\\text{pe}(x))))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47e0c635",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_src_vocab, d_word_vec, n_layers, n_head, \n",
    "                 d_k, d_v, d_model, d_ff, pad_idx, dropout=0.1, \n",
    "                 n_position=200, scale_emb=False):\n",
    "        super().__init__()\n",
    "        self.src_word_emb = nn.Embedding(n_src_vocab, d_word_vec, padding_idx=pad_idx)\n",
    "        self.position_enc = PositionalEncoding(d_word_vec, n_position=n_position)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            EncoderLayer(d_model, d_ff, n_head, d_k, d_v, dropout=dropout)\n",
    "            for _ in range(n_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.scale_emb = scale_emb\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, src_seq, src_mask, return_attns=False):\n",
    "        enc_slf_attn_list = []\n",
    "        enc_output = self.src_word_emb(src_seq)\n",
    "        if self.scale_emb:\n",
    "            enc_output *= self.d_model ** 0.5\n",
    "        enc_output = self.dropout(self.position_enc(enc_output))\n",
    "        enc_output = self.layer_norm(enc_output)\n",
    "        \n",
    "        for enc_layer in self.layer_stack:\n",
    "            enc_output, enc_slf_attn = enc_layer(enc_output, slf_attn_mask=src_mask)\n",
    "            enc_slf_attn_list += [enc_slf_attn] if return_attns else []\n",
    "            \n",
    "        if return_attns:\n",
    "            return enc_output, enc_slf_attn_list\n",
    "        return enc_output,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058fb6dc",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "$$\\text{Decoder}(x)=\\text{DecoderLayer}_N(\\dots (\\text{DecoderLayer}_1(x+\\text{pe}(x))))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9be303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "            self, n_trg_vocab, d_word_vec, n_layers, n_head, d_k, d_v,\n",
    "            d_model, d_ff, pad_idx, n_position=200, dropout=0.1, scale_emb=False):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.trg_word_emb = nn.Embedding(n_trg_vocab, d_word_vec, padding_idx=pad_idx)\n",
    "        self.position_enc = PositionalEncoding(d_word_vec, n_position=n_position)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layer_stack = nn.ModuleList([\n",
    "            DecoderLayer(d_model, d_ff, n_head, d_k, d_v, dropout=dropout)\n",
    "            for _ in range(n_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.scale_emb = scale_emb\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, trg_seq, trg_mask, enc_output, src_mask, return_attns=False):\n",
    "\n",
    "        dec_slf_attn_list, dec_enc_attn_list = [], []\n",
    "\n",
    "        # -- Forward\n",
    "        dec_output = self.trg_word_emb(trg_seq)\n",
    "        if self.scale_emb:\n",
    "            dec_output *= self.d_model ** 0.5\n",
    "        dec_output = self.dropout(self.position_enc(dec_output))\n",
    "        dec_output = self.layer_norm(dec_output)\n",
    "\n",
    "        for dec_layer in self.layer_stack:\n",
    "            dec_output, dec_slf_attn, dec_enc_attn = dec_layer(\n",
    "                dec_output, enc_output, slf_attn_mask=trg_mask, dec_enc_attn_mask=src_mask)\n",
    "            dec_slf_attn_list += [dec_slf_attn] if return_attns else []\n",
    "            dec_enc_attn_list += [dec_enc_attn] if return_attns else []\n",
    "\n",
    "        if return_attns:\n",
    "            return dec_output, dec_slf_attn_list, dec_enc_attn_list\n",
    "        return dec_output,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0164f72a",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37fb82e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_mask(seq, pad_idx):\n",
    "    return (seq != pad_idx).unsqueeze(-2)\n",
    "\n",
    "def get_subsequent_mask(seq):\n",
    "    ''' For masking out the subsequent info. '''\n",
    "    sz_b, len_s = seq.size()\n",
    "    subsequent_mask = (1 - torch.triu(\n",
    "        torch.ones((1, len_s, len_s), device=seq.device), diagonal=1)).bool()\n",
    "    return subsequent_mask\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "            self, n_src_vocab, n_trg_vocab, src_pad_idx, trg_pad_idx,\n",
    "            d_word_vec=512, d_model=512, d_ff=2048,\n",
    "            n_layers=6, n_head=8, d_k=64, d_v=64, dropout=0.1, n_position=200,\n",
    "            trg_emb_prj_weight_sharing=True, emb_src_trg_weight_sharing=True,\n",
    "            scale_emb_or_prj='prj'):\n",
    "        super().__init__()\n",
    "        self.src_pad_idx, self.trg_pad_idx = src_pad_idx, trg_pad_idx\n",
    "        \n",
    "        assert scale_emb_or_prj in ['emb', 'prj', 'none']\n",
    "        scale_emb = (scale_emb_or_prj == 'emb') if trg_emb_prj_weight_sharing else False\n",
    "        self.scale_prj = (scale_emb_or_prj == 'prj') if trg_emb_prj_weight_sharing else False\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.encoder = Encoder(\n",
    "            n_src_vocab=n_src_vocab, n_position=n_position,\n",
    "            d_word_vec=d_word_vec, d_model=d_model, d_ff=d_ff,\n",
    "            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,\n",
    "            pad_idx=src_pad_idx, dropout=dropout, scale_emb=scale_emb)\n",
    "        \n",
    "        self.decoder = Decoder(\n",
    "            n_trg_vocab=n_trg_vocab, n_position=n_position,\n",
    "            d_word_vec=d_word_vec, d_model=d_model, d_ff=d_ff,\n",
    "            n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v,\n",
    "            pad_idx=trg_pad_idx, dropout=dropout, scale_emb=scale_emb)\n",
    "        \n",
    "        self.trg_word_prj = nn.Linear(d_model, n_trg_vocab, bias=False)\n",
    "        \n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p) \n",
    "        \n",
    "        assert d_model == d_word_vec, \\\n",
    "        'To facilitate the residual connections, \\\n",
    "         the dimensions of all module outputs shall be the same.'\n",
    "        \n",
    "        # In our model, we share the same weight matrix between \n",
    "        # the two embedding layers and the pre-softmax linear transformation\n",
    "        if trg_emb_prj_weight_sharing:\n",
    "            self.trg_word_prj.weight = self.decoder.trg_word_emb.weight\n",
    "        if emb_src_trg_weight_sharing:\n",
    "            self.encoder.src_word_emb.weight = self.decoder.trg_word_emb.weight\n",
    "            \n",
    "    def forward(self, src_seq, trg_seq):\n",
    "        src_mask = get_pad_mask(src_seq, self.src_pad_idx)\n",
    "        trg_mask = get_pad_mask(trg_seq, self.trg_pad_idx) & get_subsequent_mask(trg_seq)\n",
    "\n",
    "        enc_output, *_ = self.encoder(src_seq, src_mask)\n",
    "        dec_output, *_ = self.decoder(trg_seq, trg_mask, enc_output, src_mask)\n",
    "        seq_logit = self.trg_word_prj(dec_output)\n",
    "        if self.scale_prj:\n",
    "            seq_logit *= self.d_model ** -0.5\n",
    "        \n",
    "        return seq_logit.view(-1, seq_logit.size(2))\n",
    "#         return seq_logit, enc_output, dec_output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d990f588",
   "metadata": {},
   "source": [
    "## Model Training for Copy-Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbce66ba",
   "metadata": {},
   "source": [
    "#### Copy-task data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7cf5eb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(n_vocab, n_words, batch_size, n_batch, pad_idx=0, bos_idx=1, eos_idx=2):\n",
    "    \"Generate random data for a src-tgt copy task.\"\n",
    "    data = []\n",
    "    for i in range(n_batch):\n",
    "        batch = torch.randint(3, n_vocab, size=(batch_size, n_words))\n",
    "        batch[:, 0] = bos_idx\n",
    "        eos_pos = torch.randint(1,n_words,(batch_size,))\n",
    "        for j, row in enumerate(batch):\n",
    "            row[eos_pos[j]] = eos_idx\n",
    "            row[eos_pos[j]+1:] = pad_idx\n",
    "        src = batch.requires_grad_(False).clone().detach()\n",
    "        trg = batch.requires_grad_(False).clone().detach()\n",
    "        data.append((src, trg))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a88ccb55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([[1, 6, 3, 5, 4, 5, 9, 6, 7, 2]]),\n",
       "  tensor([[1, 6, 3, 5, 4, 5, 9, 6, 7, 2]]))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gen(10, 10, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b67863",
   "metadata": {},
   "source": [
    "#### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb21ef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(pred, label, pad_idx, smoothing=False):\n",
    "    if smoothing:\n",
    "        eps = 0.1\n",
    "        n_class = pred.size(1)\n",
    "\n",
    "        one_hot = torch.zeros_like(pred).scatter(1, label.view(-1, 1), 1)\n",
    "        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n",
    "        log_prb = F.log_softmax(pred, dim=1)\n",
    "\n",
    "        non_pad_mask = label.ne(pad_idx)\n",
    "        loss = -(one_hot * log_prb).sum(dim=1)\n",
    "        loss = loss.masked_select(non_pad_mask).sum()  # average later\n",
    "    else:\n",
    "        loss = F.cross_entropy(pred, label, ignore_index=pad_idx, reduction='sum')\n",
    "    return loss\n",
    "\n",
    "def compute_performance(pred, label, pad_idx):\n",
    "    pred = pred.max(-1)[1]\n",
    "    non_pad_mask = label.ne(pad_idx)\n",
    "    n_correct = pred.eq(label).masked_select(non_pad_mask).sum().item()\n",
    "    n_word = non_pad_mask.sum().item()\n",
    "    return n_correct, n_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a42e6d",
   "metadata": {},
   "source": [
    "#### Learning rate scheduling\n",
    "$$lr = d^{-0.5}_{model} \\cdot \\min (step\\_num^{-0.5}, step\\_num \\cdot warmup\\_step^{-1.5})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61c139df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScheduledOptim():\n",
    "    def __init__(self, optimizer, lr_mul, d_model, n_warmup_steps):\n",
    "        self._optimizer = optimizer\n",
    "        self.lr_mul = lr_mul\n",
    "        self.d_model = d_model\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_steps = 0\n",
    "    \n",
    "    def step_and_update_lr(self):\n",
    "        \"Step with the inner optimizer\"\n",
    "        self._update_learning_rate()\n",
    "        self._optimizer.step()\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients with the inner optimizer\"\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    def _get_lr_scale(self):\n",
    "        d_model = self.d_model\n",
    "        n_steps, n_warmup_steps = self.n_steps, self.n_warmup_steps\n",
    "        return (d_model ** -0.5) * min(n_steps ** (-0.5), n_steps * n_warmup_steps ** (-1.5))\n",
    "\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        ''' Learning rate scheduling per step '''\n",
    "\n",
    "        self.n_steps += 1\n",
    "        lr = self.lr_mul * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "64312123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Train loss Val loss   Train acc. Val acc.   lr\n",
      "[ Epoch 0 ]   1.451351   1.314187   0.381518   0.404892   0.000056\n",
      "[ Epoch 1 ]   1.288287   1.219312   0.424756   0.453817   0.000112\n",
      "[ Epoch 2 ]   1.215812   1.170164   0.493853   0.540278   0.000168\n",
      "[ Epoch 3 ]   1.143535   1.011304   0.564646   0.679460   0.000224\n",
      "[ Epoch 4 ]   0.911118   0.787244   0.789318   0.837199   0.000280\n",
      "[ Epoch 5 ]   0.802743   0.706080   0.823654   0.878111   0.000335\n",
      "[ Epoch 6 ]   0.710047   0.636183   0.866469   0.902151   0.000391\n",
      "[ Epoch 7 ]   0.649064   0.561659   0.878762   0.900464   0.000447\n",
      "[ Epoch 8 ]   0.558382   0.442824   0.927088   0.985660   0.000503\n",
      "[ Epoch 9 ]   0.435248   0.366074   0.985587   1.000000   0.000559\n",
      "[ Epoch 10 ]   0.365624   0.315730   0.996185   1.000000   0.000615\n",
      "[ Epoch 11 ]   0.357352   0.274855   0.976685   1.000000   0.000671\n",
      "[ Epoch 12 ]   0.274382   0.230087   0.995337   1.000000   0.000727\n",
      "[ Epoch 13 ]   0.257938   0.191282   0.986859   1.000000   0.000783\n",
      "[ Epoch 14 ]   0.188302   0.157755   1.000000   1.000000   0.000839\n",
      "[ Epoch 15 ]   0.155076   0.129292   1.000000   1.000000   0.000894\n",
      "[ Epoch 16 ]   0.127746   0.104682   0.999576   1.000000   0.000950\n",
      "[ Epoch 17 ]   0.101782   0.083663   1.000000   1.000000   0.001006\n",
      "[ Epoch 18 ]   0.081039   0.066133   1.000000   1.000000   0.001062\n",
      "[ Epoch 19 ]   0.063785   0.051824   1.000000   1.000000   0.001118\n",
      "[ Epoch 20 ]   0.049861   0.040365   1.000000   1.000000   0.001174\n",
      "[ Epoch 21 ]   0.038762   0.031288   1.000000   1.000000   0.001230\n",
      "[ Epoch 22 ]   0.029994   0.024165   1.000000   1.000000   0.001286\n",
      "[ Epoch 23 ]   0.023138   0.018661   1.000000   1.000000   0.001342\n",
      "[ Epoch 24 ]   0.017813   0.014373   1.000000   1.000000   0.001398\n",
      "[ Epoch 25 ]   0.013704   0.011080   1.000000   1.000000   0.001453\n",
      "[ Epoch 26 ]   0.010537   0.008530   1.000000   1.000000   0.001509\n",
      "[ Epoch 27 ]   0.008101   0.006583   1.000000   1.000000   0.001565\n",
      "[ Epoch 28 ]   0.006234   0.005082   1.000000   1.000000   0.001621\n",
      "[ Epoch 29 ]   0.004803   0.003924   1.000000   1.000000   0.001677\n",
      "[ Epoch 30 ]   0.003702   0.003031   1.000000   1.000000   0.001733\n",
      "[ Epoch 31 ]   0.002859   0.002348   1.000000   1.000000   0.001789\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [69]\u001b[0m, in \u001b[0;36m<cell line: 93>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[ Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ]\u001b[39m\u001b[38;5;124m'\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m10.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalidation_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m10.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m10.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalidation_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m10.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m 10.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [69]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, device)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m              Train loss Val loss   Train acc. Val acc.   lr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epoch):\n\u001b[0;32m---> 87\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     validation_loss, validation_acc \u001b[38;5;241m=\u001b[39m eval_epoch(model, val_data, device)\n\u001b[1;32m     89\u001b[0m     lr \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39m_optimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Input \u001b[0;32mIn [69]\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_data, optimizer, device, smoothing)\u001b[0m\n\u001b[1;32m     44\u001b[0m trg_seq \u001b[38;5;241m=\u001b[39m trg_seq\u001b[38;5;241m.\u001b[39mto(device)[:,\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     45\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 46\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m label \u001b[38;5;241m=\u001b[39m trg_seq\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     48\u001b[0m loss \u001b[38;5;241m=\u001b[39m compute_loss(pred, label, pad_idx, smoothing\u001b[38;5;241m=\u001b[39msmoothing)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src_seq, trg_seq)\u001b[0m\n\u001b[1;32m     57\u001b[0m trg_mask \u001b[38;5;241m=\u001b[39m get_pad_mask(trg_seq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrg_pad_idx) \u001b[38;5;241m&\u001b[39m get_subsequent_mask(trg_seq)\n\u001b[1;32m     59\u001b[0m enc_output, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(src_seq, src_mask)\n\u001b[0;32m---> 60\u001b[0m dec_output, \u001b[38;5;241m*\u001b[39m_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrg_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m seq_logit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrg_word_prj(dec_output)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_prj:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, trg_seq, trg_mask, enc_output, src_mask, return_attns)\u001b[0m\n\u001b[1;32m     27\u001b[0m dec_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(dec_output)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dec_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_stack:\n\u001b[0;32m---> 30\u001b[0m     dec_output, dec_slf_attn, dec_enc_attn \u001b[38;5;241m=\u001b[39m \u001b[43mdec_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdec_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslf_attn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrg_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_enc_attn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     dec_slf_attn_list \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [dec_slf_attn] \u001b[38;5;28;01mif\u001b[39;00m return_attns \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[1;32m     33\u001b[0m     dec_enc_attn_list \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [dec_enc_attn] \u001b[38;5;28;01mif\u001b[39;00m return_attns \u001b[38;5;28;01melse\u001b[39;00m []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[0;34m(self, dec_input, enc_output, slf_attn_mask, dec_enc_attn_mask)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, dec_input, enc_output, slf_attn_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dec_enc_attn_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m----> 9\u001b[0m     dec_output, dec_slf_attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslf_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdec_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mslf_attn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     dec_output, dec_enc_attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc_attn(\n\u001b[1;32m     12\u001b[0m         dec_output, enc_output, enc_output, mask\u001b[38;5;241m=\u001b[39mdec_enc_attn_mask)\n\u001b[1;32m     13\u001b[0m     dec_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_ffn(dec_output)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m     37\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m attn_out\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(batch_size, len_q, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Change shape from (b, h, n, d_v) to (b, n, h*d_v)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m mh_attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(attn_out) \u001b[38;5;66;03m# (b, n, d_model)\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmh_attn_out\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, attn\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/normalization.py:189\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_vocab = 5\n",
    "n_words = 5\n",
    "d_model = 128\n",
    "n_batch = 20\n",
    "batch_size = 48\n",
    "n_layers, n_head = 6, 8, \n",
    "d_k, d_v = d_model//n_head, d_model//n_head\n",
    "\n",
    "pad_idx = 0\n",
    "bos_idx = 1\n",
    "eos_idx = 2\n",
    "\n",
    "n_warmup_steps = 1000\n",
    "n_epoch = 20\n",
    "\n",
    "# Model\n",
    "model = Transformer(\n",
    "    n_src_vocab=n_vocab, n_trg_vocab=n_vocab, \n",
    "    src_pad_idx=pad_idx, trg_pad_idx=pad_idx,\n",
    "    d_word_vec=d_model, d_model=d_model, d_ff=d_model*4,\n",
    "    n_layers=n_layers, n_head=n_head, d_k=d_k, d_v=d_v)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-09)\n",
    "optimizer = ScheduledOptim(optimizer, 1.0, d_model, n_warmup_steps)\n",
    "\n",
    "# Device\n",
    "device = \"cpu\"\n",
    "\n",
    "# Data\n",
    "# train_data = [(torch.tensor([[4,4,2,0]]), torch.tensor([[4,4,2,0]]))]\n",
    "train_data = data_gen(n_vocab, n_words, batch_size, n_batch)\n",
    "val_data = data_gen(n_vocab, n_words, batch_size, n_batch)\n",
    "\n",
    "# print(f\"train_data: {train_data}\")\n",
    "# print(f\"val_data: {val_data}\")\n",
    "\n",
    "def train_epoch(model, train_data, optimizer, device, smoothing):\n",
    "    model.train()\n",
    "    total_loss, n_word_total, n_word_correct = 0, 0, 0 \n",
    "    \n",
    "    for src_seq, trg_seq in train_data:\n",
    "        src_seq = src_seq.to(device)\n",
    "        trg_seq = trg_seq.to(device)[:,1:]\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(src_seq, trg_seq)\n",
    "        label = trg_seq.contiguous().view(-1)\n",
    "        loss = compute_loss(pred, label, pad_idx, smoothing=smoothing)\n",
    "        loss.backward()\n",
    "        optimizer.step_and_update_lr()\n",
    "        n_correct, n_word = compute_performance(pred, label, pad_idx)\n",
    "        n_word_correct += n_correct\n",
    "        n_word_total += n_word\n",
    "        \n",
    "#         n_word_correct += (pred.max(-1)[1] == label).sum()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    loss_per_word = total_loss/n_word_total\n",
    "    accuracy = n_word_correct/n_word_total\n",
    "    return loss_per_word, accuracy\n",
    "\n",
    "def eval_epoch(model, val_data, device):\n",
    "    model.eval()\n",
    "    total_loss, n_word_total, n_word_correct = 0, 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for src_seq, trg_seq in val_data:\n",
    "            src_seq = src_seq.to(device)\n",
    "            trg_seq = trg_seq.to(device)[:,1:]\n",
    "            pred = model(src_seq, trg_seq)\n",
    "            label = trg_seq.contiguous().view(-1)\n",
    "            loss = compute_loss(pred, label, pad_idx, smoothing=False)\n",
    "#             n_word_total += len(label)\n",
    "#             n_word_correct += (pred.max(-1)[1] == label).sum()\n",
    "            n_correct, n_word = compute_performance(pred, label, pad_idx)\n",
    "            n_word_correct += n_correct\n",
    "            n_word_total += n_word\n",
    "            total_loss += loss.item()\n",
    "    loss_per_word = total_loss/n_word_total\n",
    "    accuracy = n_word_correct/n_word_total\n",
    "    return loss_per_word, accuracy\n",
    "\n",
    "def train(model, optimizer, device):\n",
    "    print(\"              Train loss Val loss   Train acc. Val acc.   lr\")\n",
    "    for epoch in range(n_epoch):\n",
    "        train_loss, train_acc = train_epoch(model, train_data, optimizer, device, smoothing=False)\n",
    "        validation_loss, validation_acc = eval_epoch(model, val_data, device)\n",
    "        lr = optimizer._optimizer.param_groups[0]['lr']\n",
    "        print(f'[ Epoch {epoch} ]', end=\" \")\n",
    "        print(f\"{train_loss:10.6f} {validation_loss:10.6f} {train_acc:10.6f} {validation_acc:10.6f} {lr: 10.6f}\")\n",
    "\n",
    "train(model, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118b24f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dac082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471fab74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2827e5bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11ec2fef",
   "metadata": {},
   "source": [
    "## Copy-task Translation with Beam Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "297d37b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(nn.Module):\n",
    "    def __init__(\n",
    "            self, model, beam_size, max_seq_len,\n",
    "            src_pad_idx, trg_pad_idx, trg_bos_idx, trg_eos_idx):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.alpha = 0.7\n",
    "        self.beam_size = beam_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_bos_idx = trg_bos_idx\n",
    "        self.trg_eos_idx = trg_eos_idx\n",
    "        \n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.register_buffer('init_seq', torch.LongTensor([[trg_bos_idx]]))\n",
    "        self.register_buffer(\n",
    "            'blank_seqs', \n",
    "            torch.full((beam_size, max_seq_len), trg_pad_idx, dtype=torch.long))\n",
    "        self.blank_seqs[:, 0] = self.trg_bos_idx\n",
    "        self.register_buffer(\n",
    "            'len_map', \n",
    "            torch.arange(1, max_seq_len + 1, dtype=torch.long).unsqueeze(0))\n",
    "        \n",
    "    def _get_init_state(self, src_seq, src_mask):\n",
    "        beam_size = self.beam_size\n",
    "        enc_output, *_ = self.model.encoder(src_seq, src_mask)\n",
    "        dec_output = self._model_decode(self.init_seq, enc_output, src_mask)\n",
    "        \n",
    "        best_k_probs, best_k_idx = dec_output[:, -1, :].topk(beam_size)\n",
    "        \n",
    "        scores = torch.log(best_k_probs).view(beam_size)\n",
    "        gen_seq = self.blank_seqs.clone().detach()\n",
    "        gen_seq[:, 1] = best_k_idx[0]\n",
    "        enc_output = enc_output.repeat(beam_size, 1, 1)\n",
    "        return enc_output, gen_seq, scores\n",
    "    \n",
    "    def _model_decode(self, trg_seq, enc_output, src_mask):\n",
    "        trg_mask = get_subsequent_mask(trg_seq)\n",
    "        dec_output, *_ = self.model.decoder(trg_seq, trg_mask, enc_output, src_mask)\n",
    "        return F.softmax(self.model.trg_word_prj(dec_output), dim=-1)\n",
    "\n",
    "    def _get_the_best_score_and_idx(self, gen_seq, dec_output, scores, step):\n",
    "        assert len(scores.size()) == 1\n",
    "        beam_size = self.beam_size\n",
    "\n",
    "        # Get k candidates for each beam, k^2 candidates in total.\n",
    "        best_k2_probs, best_k2_idx = dec_output[:, -1, :].topk(beam_size)\n",
    "\n",
    "        # Include the previous scores.\n",
    "        scores = torch.log(best_k2_probs).view(beam_size, -1) + scores.view(beam_size, 1)\n",
    "\n",
    "        # Get the best k candidates from k^2 candidates.\n",
    "        scores, best_k_idx_in_k2 = scores.view(-1).topk(beam_size)\n",
    " \n",
    "        # Get the corresponding positions of the best k candidiates.\n",
    "        best_k_r_idxs, best_k_c_idxs = torch.div(best_k_idx_in_k2, beam_size, rounding_mode=\"floor\"), best_k_idx_in_k2 % beam_size\n",
    "#         best_k_r_idxs, best_k_c_idxs = best_k_idx_in_k2 // beam_size, best_k_idx_in_k2 % beam_size\n",
    "        best_k_idx = best_k2_idx[best_k_r_idxs, best_k_c_idxs]\n",
    "\n",
    "        # Copy the corresponding previous tokens.\n",
    "        gen_seq[:, :step] = gen_seq[best_k_r_idxs, :step]\n",
    "        # Set the best tokens in this beam search step\n",
    "        gen_seq[:, step] = best_k_idx\n",
    "\n",
    "        return gen_seq, scores\n",
    "\n",
    "    def translate(self, src_seq):\n",
    "        assert src_seq.size(0) == 1\n",
    "        src_pad_idx, trg_eos_idx = self.src_pad_idx, self.trg_eos_idx \n",
    "        max_seq_len, beam_size, alpha = self.max_seq_len, self.beam_size, self.alpha \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            src_mask = get_pad_mask(src_seq, src_pad_idx)\n",
    "            enc_output, gen_seq, scores = self._get_init_state(src_seq, src_mask)\n",
    "            \n",
    "            ans_idx = 0\n",
    "            for step in range(2, max_seq_len): \n",
    "                dec_output = self._model_decode(gen_seq[:, :step], enc_output, src_mask)\n",
    "                gen_seq, scores = self._get_the_best_score_and_idx(gen_seq, dec_output, scores, step)\n",
    "                print(f\"Step {step}: gen_seq = {gen_seq}\")\n",
    "                eos_locs = gen_seq == trg_eos_idx  \n",
    "                seq_lens, _ = self.len_map.masked_fill(~eos_locs, max_seq_len).min(1)\n",
    "                if (eos_locs.sum(1) > 0).sum(0).item() == beam_size:\n",
    "                    _, ans_idx = scores.div(seq_lens.float() ** alpha).max(0)\n",
    "                    ans_idx = ans_idx.item()\n",
    "                    break\n",
    "        return gen_seq[ans_idx][:seq_lens[ans_idx]].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "874e8b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[1, 3, 3, 4, 2]]), tensor([[1, 3, 3, 4, 2]]))\n",
      "Step 2: gen_seq = tensor([[1, 4, 4, 0, 0]])\n",
      "Step 3: gen_seq = tensor([[1, 4, 4, 4, 0]])\n",
      "Step 4: gen_seq = tensor([[1, 4, 4, 4, 4]])\n",
      "[1, 4, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "translator = Translator(model, beam_size=1, max_seq_len=n_words, \n",
    "           src_pad_idx=pad_idx, trg_pad_idx=pad_idx,\n",
    "           trg_bos_idx=bos_idx, trg_eos_idx=eos_idx)\n",
    "test_sentence = data_gen(n_vocab, n_words, 1, 1)[0]\n",
    "# test_sentence = train_data[0]\n",
    "print(test_sentence)\n",
    "print(translator.translate(test_sentence[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d6ce9c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 3, 6, 6, 3, 5, 3, 7, 2, 0]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "56d01a43",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'trg_seq'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [85]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m src_seq \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]])\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_seq\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'trg_seq'"
     ]
    }
   ],
   "source": [
    "src_seq = torch.tensor([[1, 7, 4, 2, 0, 0, 0, 0, 0, 0]])\n",
    "model(src_seq, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625ba889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abd66da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bee6ac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "220a8b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Train loss Val loss   Train acc. Val acc.   lr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Epoch 0 ]   1.748914   1.709326   0.332925   0.363215   0.000263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Epoch 1 ]   1.705879   1.683732   0.354998   0.356336   0.000319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Epoch 2 ]   1.689812   1.660596   0.344968   0.356722   0.000375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Epoch 3 ]   1.680491   1.676481   0.338462   0.327337   0.000430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Epoch 4 ]   1.674520   1.652499   0.318880   0.333809   0.000486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Epoch 5 ]   1.638099   1.648261   0.340865   0.334579   0.000542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Epoch 6 ]   1.636380   1.611258   0.320467   0.343630   0.000598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Epoch 7 ]   1.609070   1.593879   0.339497   0.328020   0.000654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Epoch 8 ]   1.584795   1.557871   0.334279   0.354617   0.000710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Epoch 9 ]   1.823320   1.598386   0.269399   0.313562   0.000766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "pad_idx = 0\n",
    "device = \"cpu\"\n",
    "n_vocab = 8\n",
    "n_words = 10\n",
    "d_model = 128\n",
    "n_batch = 20\n",
    "batch_size = 48\n",
    "\n",
    "def cal_performance(pred, gold, trg_pad_idx, smoothing=False):\n",
    "    ''' Apply label smoothing if needed '''\n",
    "\n",
    "    loss = cal_loss(pred, gold, trg_pad_idx, smoothing=smoothing)\n",
    "\n",
    "    pred = pred.max(1)[1]\n",
    "    gold = gold.contiguous().view(-1)\n",
    "    non_pad_mask = gold.ne(trg_pad_idx)\n",
    "    n_correct = pred.eq(gold).masked_select(non_pad_mask).sum().item()\n",
    "    n_word = non_pad_mask.sum().item()\n",
    "\n",
    "    return loss, n_correct, n_word\n",
    "\n",
    "def eval_epoch(model, device, pad_idx):\n",
    "    ''' Epoch operation in evaluation phase '''\n",
    "\n",
    "    model.eval()\n",
    "    total_loss, n_word_total, n_word_correct = 0, 0, 0\n",
    "    validation_data = data_gen(n_vocab, n_words, batch_size, n_batch)\n",
    "\n",
    "    desc = '  - (Validation) '\n",
    "    with torch.no_grad():\n",
    "        for src, trg in tqdm(validation_data, mininterval=2, desc=desc, leave=False):\n",
    "\n",
    "            # prepare data\n",
    "            src_seq = patch_src(src, pad_idx).to(device)\n",
    "            trg_seq, gold = map(lambda x: x.to(device), patch_trg(trg, pad_idx))\n",
    "\n",
    "            # forward\n",
    "            pred = model(src_seq, trg_seq)\n",
    "            loss, n_correct, n_word = cal_performance(\n",
    "                pred, gold, pad_idx, smoothing=False)\n",
    "\n",
    "            # note keeping\n",
    "            n_word_total += n_word\n",
    "            n_word_correct += n_correct\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    loss_per_word = total_loss/n_word_total\n",
    "    accuracy = n_word_correct/n_word_total\n",
    "    return loss_per_word, accuracy\n",
    "\n",
    "def cal_loss(pred, gold, trg_pad_idx, smoothing=False):\n",
    "    ''' Calculate cross entropy loss, apply label smoothing if needed. '''\n",
    "\n",
    "    gold = gold.contiguous().view(-1)\n",
    "\n",
    "    if smoothing:\n",
    "        eps = 0.1\n",
    "        n_class = pred.size(1)\n",
    "\n",
    "        one_hot = torch.zeros_like(pred).scatter(1, gold.view(-1, 1), 1)\n",
    "        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n",
    "        log_prb = F.log_softmax(pred, dim=1)\n",
    "\n",
    "        non_pad_mask = gold.ne(trg_pad_idx)\n",
    "        loss = -(one_hot * log_prb).sum(dim=1)\n",
    "        loss = loss.masked_select(non_pad_mask).sum()  # average later\n",
    "    else:\n",
    "        loss = F.cross_entropy(pred, gold, ignore_index=trg_pad_idx, reduction='sum')\n",
    "    return loss\n",
    "\n",
    "\n",
    "def patch_src(src, pad_idx):\n",
    "    src = src.transpose(0, 1)\n",
    "    return src\n",
    "\n",
    "\n",
    "def patch_trg(trg, pad_idx):\n",
    "    trg = trg.transpose(0, 1)\n",
    "    trg, gold = trg[:, :-1], trg[:, 1:].contiguous().view(-1)\n",
    "    return trg, gold\n",
    "\n",
    "def train_epoch(model, optimizer, device, smoothing):\n",
    "    ''' Epoch operation in training phase'''\n",
    "\n",
    "    model.train()\n",
    "    total_loss, n_word_total, n_word_correct = 0, 0, 0 \n",
    "    training_data = data_gen(n_vocab, n_words, batch_size, n_batch)\n",
    "\n",
    "    desc = '  - (Training)   '\n",
    "    for src, trg in tqdm(training_data, mininterval=2, desc=desc, leave=False):\n",
    "\n",
    "        # prepare data\n",
    "        src_seq = patch_src(src, pad_idx).to(device)\n",
    "        trg_seq, gold = map(lambda x: x.to(device), patch_trg(trg, pad_idx))\n",
    "\n",
    "        # forward\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(src_seq, trg_seq)\n",
    "\n",
    "        # backward and update parameters\n",
    "        loss, n_correct, n_word = cal_performance(\n",
    "            pred, gold, pad_idx, smoothing=smoothing) \n",
    "        loss.backward()\n",
    "        optimizer.step_and_update_lr()\n",
    "\n",
    "        # note keeping\n",
    "        n_word_total += n_word\n",
    "        n_word_correct += n_correct\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    loss_per_word = total_loss/n_word_total\n",
    "    accuracy = n_word_correct/n_word_total\n",
    "    return loss_per_word, accuracy\n",
    "\n",
    "def train(model, optimizer, device):\n",
    "    print(\"              Train loss Val loss   Train acc. Val acc.   lr\")\n",
    "    for epoch in range(n_epoch):\n",
    "        train_loss, train_acc = train_epoch(model, optimizer, device, smoothing=False)\n",
    "        validation_loss, validation_acc = eval_epoch(model, device, pad_idx)\n",
    "        lr = optimizer._optimizer.param_groups[0]['lr']\n",
    "        print(f'[ Epoch {epoch} ]', end=\" \")\n",
    "        print(f\"{train_loss:10.6f} {validation_loss:10.6f} {train_acc:10.6f} {validation_acc:10.6f} {lr: 10.6f}\")\n",
    "\n",
    "train(model, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a50c353",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
